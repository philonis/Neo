import streamlit as st
from llm_client import LLMClient
from tools.notes_skill import NotesSkill
from tools.memory_skill import PersonalMemorySkill
import json

# 1. é¡µé¢åŸºç¡€é…ç½®
st.set_page_config(page_title="æœ¬åœ°æ™ºèƒ½åŠ©æ‰‹", page_icon="ğŸ¤–", layout="centered")

# 2. åˆå§‹åŒ–èµ„æº (ä½¿ç”¨ st.cache_resource ç¡®ä¿å…¨å±€åªåŠ è½½ä¸€æ¬¡)
@st.cache_resource
def init_resources():
    client = LLMClient()
    memory = PersonalMemorySkill()
    return client, memory

client, memory = init_resources()

# æ³¨å†Œå·¥å…·
available_functions = {
    "notes_operator": NotesSkill.run,
}
tools_schema = [NotesSkill.get_tool_definition()]

# 3. åˆå§‹åŒ– Session State (å¯¹è¯å†å²)
# è¿™é‡Œçš„ messages æ˜¯å‘é€ç»™ LLM çš„å®Œæ•´å†å²
if "messages" not in st.session_state:
    # åŠ è½½è®°å¿†ä¸Šä¸‹æ–‡
    base_system_prompt = "ä½ æ˜¯ä¸€ä¸ªè¿è¡Œåœ¨ç”¨æˆ· macOS ä¸Šçš„æœ¬åœ°åŠ©æ‰‹ã€‚ä½ å¯ä»¥æ“ä½œå¤‡å¿˜å½•ç­‰æœ¬åœ°åº”ç”¨ã€‚"
    memory_context = memory.load_context()
    
    st.session_state.messages = [
        {"role": "system", "content": base_system_prompt + memory_context}
    ]
    
    # å†å²èŠå¤©è®°å½• (ä»…ç”¨äºé¡µé¢æ˜¾ç¤ºï¼Œä¸åŒ…å« System Prompt)
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

# 4. é¡µé¢æ ‡é¢˜
st.title("ğŸ¤– æœ¬åœ°æ™ºèƒ½åŠ©æ‰‹")
st.caption("æ”¯æŒå¤‡å¿˜å½•æ“ä½œ & é•¿æœŸè®°å¿† | Powered by Streamlit")

# 5. æ˜¾ç¤ºå†å²èŠå¤©è®°å½• (é¡µé¢åˆ·æ–°æ—¶æ¸²æŸ“)
for message in st.session_state.chat_history:
    # è¿‡æ»¤æ‰ tool å’Œ system æ¶ˆæ¯ï¼Œåªæ˜¾ç¤º user å’Œ assistant
    if message["role"] in ["user", "assistant"]:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# 6. å¤„ç†ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æŒ‡ä»¤..."):
    # --- æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯ ---
    st.chat_message("user").markdown(prompt)
    
    # æ·»åŠ åˆ°å†å²è®°å½•
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.session_state.chat_history.append({"role": "user", "content": prompt})

    # --- è°ƒç”¨ LLM ---
    with st.chat_message("assistant"):
        # åˆ›å»ºä¸€ä¸ªç©ºçš„å ä½ç¬¦ï¼Œç”¨äºæµå¼/é€æ­¥æ˜¾ç¤ºå†…å®¹
        message_placeholder = st.empty()
        message_placeholder.markdown("æ€è€ƒä¸­...")

        # 1. ç¬¬ä¸€æ¬¡è°ƒç”¨ LLM
        response = client.chat(st.session_state.messages, tools=tools_schema)
        
        if not response or "choices" not in response:
            message_placeholder.markdown("âŒ è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key æˆ–ç½‘ç»œã€‚")
            st.stop()

        assistant_message = response["choices"][0]["message"]
        final_reply_content = ""

        # 2. å¤„ç†å·¥å…·è°ƒç”¨
        if assistant_message.get("tool_calls"):
            # å¿…é¡»æŠŠ LLM çš„å·¥å…·è°ƒç”¨æ„å›¾åŠ å…¥å†å²ï¼Œå¦åˆ™ä¼šæ–­ç‰‡
            st.session_state.messages.append(assistant_message)
            
            # éå†å·¥å…·è°ƒç”¨
            for tool_call in assistant_message["tool_calls"]:
                func_name = tool_call["function"]["name"]
                func_args_str = tool_call["function"]["arguments"]
                
                # æ˜¾ç¤ºå·¥å…·è°ƒç”¨çŠ¶æ€ (æ¯” CLI æ›´ä¼˜é›…çš„æ–¹å¼)
                with st.status(f"ğŸ”§ æ­£åœ¨æ‰§è¡Œå·¥å…·: {func_name}...", expanded=False) as status:
                    st.write(f"å‚æ•°: {func_args_str}")
                    
                    if func_name in available_functions:
                        func_args = json.loads(func_args_str)
                        tool_result = available_functions[func_name](func_args)
                        st.write(f"ç»“æœ: {tool_result}")
                        status.update(label=f"âœ… å·¥å…· {func_name} æ‰§è¡Œå®Œæ¯•", state="complete")
                        
                        # å°†ç»“æœå›ä¼ ç»™ LLM
                        st.session_state.messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call["id"],
                            "name": func_name,
                            "content": json.dumps(tool_result, ensure_ascii=False)
                        })

            # 3. ç¬¬äºŒæ¬¡è°ƒç”¨ LLM (ç”Ÿæˆæœ€ç»ˆå›å¤)
            message_placeholder.markdown("æ­£åœ¨ç»„ç»‡è¯­è¨€...")
            final_response = client.chat(st.session_state.messages, tools=tools_schema)
            
            if final_response:
                final_reply_content = final_response["choices"][0]["message"]["content"]
            else:
                final_reply_content = "å·¥å…·æ‰§è¡Œå®Œæ¯•ï¼Œä½†ç”Ÿæˆå›å¤å¤±è´¥ã€‚"
        
        else:
            # ç›´æ¥å›å¤
            final_reply_content = assistant_message.get("content", " ")

        # --- æœ€ç»ˆæ¸²æŸ“ ---
        message_placeholder.markdown(final_reply_content)
        
        # æ›´æ–°å†å²
        st.session_state.messages.append({"role": "assistant", "content": final_reply_content})
        st.session_state.chat_history.append({"role": "assistant", "content": final_reply_content})

        # --- è®°å¿†å›ºåŒ– ---
        if final_reply_content:
            memory.record_interaction(prompt, final_reply_content, client)
